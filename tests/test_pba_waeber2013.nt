
test_posterior_median:
  -
    posterior: PosteriorDist([0, 1], [0], 0)
    x: 0.5
    x_i: 1
  -
    posterior: PosteriorDist([0, 2], [0], 0)
    x: 1
    x_i: 1
  -
    posterior: PosteriorDist([0, 0.5, 1], [0, 0], 1)
    x: 0.5
    x_i: 1
  -
    posterior: PosteriorDist([0, 0.5, 1], [log(1.25), log(0.75)], 1)
    # PDF: 0.625, 0.375
    # CDF: 0, 0.625, 1.0
    x: 0.4
    x_i: 1
  -
    posterior: PosteriorDist([0, 0.5, 1], [log(0.75), log(1.25)], 1)
    # PDF: 0.375, 0.625
    # CDF: 0, 0.375, 1.0
    x: 0.6
    x_i: 2

test_posterior_update:
  -
    id: base-1
    posterior: PosteriorDist([0, 1, 0], [0, 0], 0)
    x: 0.5
    x_i: 1
    sign_guess: 1
    p_c: 0.8
    slope: 1
    expected:
      xs: [0, 0.5, 1]
      ps: [1.6, 0.4]
      n: 1
  -
    id: base-2
    posterior: PosteriorDist([0, 0.5, 1, 0], [log(1.6), log(0.4), 0], 1)
    x: 0.3125
    x_i: 1
    sign_guess: 1
    p_c: 0.8
    slope: 1
    expected:
      xs: [0, 0.3125, 0.5, 1]
      ps: [2.56, 0.64, 0.16]
      n: 2
  -
    id: sign_guess
    posterior: PosteriorDist([0, 1, 0], [0, 0], 0)
    x: 0.5
    x_i: 1
    sign_guess: -1
    p_c: 0.8
    slope: 1
    expected:
      xs: [0, 0.5, 1]
      ps: [0.4, 1.6]
      n: 1
  -
    id: slope
    posterior: PosteriorDist([0, 1, 0], [0, 0], 0)
    x: 0.5
    x_i: 1
    sign_guess: 1
    p_c: 0.8
    slope: -1
    expected:
      xs: [0, 0.5, 1]
      ps: [0.4, 1.6]
      n: 1
  -
    id: p_c
    posterior: PosteriorDist([0, 1, 0], [0, 0], 0)
    x: 0.5
    x_i: 1
    sign_guess: 1
    p_c: 0.7
    slope: 1
    expected:
      xs: [0, 0.5, 1]
      ps: [1.4, 0.6]
      n: 1
  -
    id: repeat-x
    posterior: PosteriorDist([0, 0.5, 1], [0, 0], 1)
    x: 0.5
    x_i: 1
    sign_guess: 1
    p_c: 0.8
    slope: 1
    expected:
      xs: [0, 0.5, 1]
      ps: [1.6, 0.4]
      n: 1
  -
    id: scale-x
    posterior: PosteriorDist([0, 2, 0], [0, 0], 0)
    x: 1
    x_i: 1
    sign_guess: 1
    p_c: 0.8
    slope: 1
    expected:
      xs: [0, 1, 2]
      ps: [1.6, 0.4]
      n: 1

test_ci_width:
  -
    ci: ConfidenceIntervals([[0],[1],[0],[1]], 1)
    width: 1
    width_seq: 1
  -
    ci: ConfidenceIntervals([[0, 0.4],[1, 0.6],[0, 0.2],[1, 0.8]], 2)
    n: 0
    width: 1
    width_seq: 1
  -
    ci: ConfidenceIntervals([[0, 0.4],[1, 0.6],[0, 0.2],[1, 0.8]], 2)
    n: 1
    width: 0.2
    width_seq: 0.6

test_ci_update:
  # I'm not really sure how to test this function, since you'd need more 
  # samples than you could calculate by hand before you'd expect to start 
  # seeing the confidence intervals narrow.  Right now I just make sure that 
  # the first interval is correct, and that the sequential intervals can only 
  # get narrower.  This doesn't test for whether or not I understood the math 
  # correctly.
  -
    id: base
    ci: ConfidenceIntervals([[0],[0],[0],[0]], 0)
    posterior: PosteriorDist([0, 0.5, 1], [1.6, 0.4], 1)
    p_c: 0.8
    alpha: 0.05
    expected: [[0], [1]]
    expected_seq: [[0], [1]]
  -
    id: always-shrink
    ci: ConfidenceIntervals([[0.1, 0],[0.9, 0],[0.1, 0],[0.9, 0]], 1)
    posterior: PosteriorDist([0, 0.5, 1], [1.6, 0.4], 1)
    p_c: 0.8
    alpha: 0.05
    expected: [[0.1, 0.0], [0.9, 1.0]]
    expected_seq: [[0.1, 0.1], [0.9, 0.9]]

test_determine_sign:
  # Use `plot_tp1.py` to see how many iterations are needed to pass the 
  # threshold for the test of power one.  This of course assumes that the 
  # equation itself is correct, but it does at least seem reasonable.
  -
    id: p=0.6
    random_walk: 3 * [1]
    p_c: 0.6
    n_max: 3
    expected: 1
  -
    id: p=0.6
    random_walk: [-1] + 6 * [1]
    p_c: 0.6
    n_max: 7
    expected: 1
  -
    id: p=0.9
    random_walk: 7 * [1]
    p_c: 0.6
    n_max: 7
    expected: 1
  -
    id: p=0.9
    random_walk: [-1] + 9 * [1]
    p_c: 0.6
    n_max: 10
    expected: 1
  -
    id: zero
    random_walk: 5 * [1, -1]
    p_c: 0.6
    n_max: 10
    expected: 0
  -
    id: ignore-magnitude
    random_walk: 5 * [100, -100]
    p_c: 0.6
    n_max: 10
    expected: 0

test_get_tol:
  -
    x: 10
    atol: 1e-6
    rtol: None
    expected: 1e-6
  -
    x: 10
    atol: None
    rtol: 1e-6
    expected: 1e-5
  -
    x: 10
    atol: None
    rtol: None
    expected: 0

test_pba_waeber2013_params:
  # I can't find parameters that converge in 100,000 iterations for 
  # any of these simple functions, although the unconverged root estimate is 
  # always very accurate.  It seems that a lot of macro iterations are needed 
  # to narrow the CI, but it doesn't take very long before the algorithm gets 
  # close to the root and gets stuck evaluating the same point over and over.
  #
  # You can set *p_c* very low to encourage the algorithm to sample more 
  # points, but if you set *p_c* too low, the algorithm won't be able to cross 
  # the root and the CI on the far side of the root will never move.
  #
  # It may be that I'm just calculating the confidence intervals wrong, but 
  # this behavior seems consistent with the numerical simulations in 
  # [Waeber2013], which all go out to 1e6 iterations.
  -
    id: linear
    f:
      > def f(x):
      >     return x
    noise: 0.1
    a: -1/3
    b: 2/3
    kwargs:
      tol: 1e-2
      slope: 1
      maxiter: 10_000
    expected: approx(0, abs=1e-2)
  -
    id: linear-tol=0
    f:
      > def f(x):
      >     return x
    noise: 0.1
    a: -1/3
    b: 2/3
    kwargs:
      tol: 0
      slope: 1
      maxiter: 10_000
    expected: approx(0, abs=1e-2)
  -
    id: linear-rtol
    f:
      > def f(x):
      >     return x - 1
    noise: 0.1
    a: 2/3
    b: 5/3
    kwargs:
      rtol: 1e-2
      slope: 1
      maxiter: 10_000
    expected: approx(1, rel=1e-2)
  -
    id: linear-alpha
    f:
      > def f(x):
      >     return x
    noise: 0.1
    a: -1/3
    b: 2/3
    kwargs:
      tol: 1e-2
      alpha: 0.2
      slope: 1
      maxiter: 10_000
    expected: approx(0, abs=1e-2)
  -
    id: linear-p_c
    f:
      > def f(x):
      >     return x
    noise: 0.1
    a: -1/3
    b: 2/3
    kwargs:
      tol: 1e-2
      p_c: 0.65
      slope: 1
      maxiter: 10_000
    expected: approx(0, abs=1e-2)
  -
    id: linear-maxiter
    f:
      > def f(x):
      >     return x
    noise: 0.1
    a: -1/3
    b: 2/3
    kwargs:
      tol: 1e-2
      slope: 1
      maxiter: 1
    expected: approx(1/6)
    converged: False
    n_obs: 1
    n_post: 1
  -
    id: linear-check-bounds
    f:
      > def f(x):
      >     return -x
    noise: 0.1
    a: -1/3
    b: 2/3
    kwargs:
      tol: 1e-2
      check_bounds: True
      maxiter: 10_000
    expected: approx(0, abs=1e-2)
  -
    id: linear-slope
    f:
      > def f(x):
      >     return -x
    noise: 0.1
    a: -1/3
    b: 2/3
    kwargs:
      tol: 1e-2
      slope: -1
      maxiter: 10_000
    expected: approx(0, abs=1e-2)
  -
    id: linear-args-kwargs
    f:
      > def f(x, a, b):
      >     return x - a - b
    noise: 0.1
    a: 1/3
    b: 4/3
    kwargs:
      tol: 1e-2
      slope: 1
      args: (1/3,)
      kwargs: {'b': 2/3}
      maxiter: 10_000
    expected: approx(1, abs=1e-2)
  -
    id: linear-guess-root
    # This is somehwat of a pathological case, since we guess the root on the 
    # first try.  That means we'll never sample any other points, and the 
    # algorithm won't converge.  Note that it's also very important for this 
    # test case that `np.sign(0) == 0`.
    f:
      > def f(x):
      >     return x
    noise: 0.1
    a: -1
    b: +1
    kwargs:
      tol: 1e-2
      slope: 1
      maxiter: 100
    expected: 0
    n_obs: 100
    n_post: 1
  -
    id: discrete
    f:
      > def f(x):
      >     return round(x) - 0.5
    noise: 0.1
    a: 1/3
    b: 4/3
    kwargs:
      tol: 1e-2
      slope: 1
      maxiter: 10_000
    expected: approx(0.5, abs=1e-2)
  -
    id: err-no-solution
    f:
      > def f(x):
      >     return x
    noise: 0.1
    a: 1
    b: 2
    kwargs:
      tol: 1e-2
      check_bounds: True
    error:
      type: RuntimeError
      message:
        - no solution
        - both bounds have the same sign: 1.0
  -
    id: err-no-slope
    f:
      > def f(x):
      >     return x
    noise: 0.1
    a: -1/3
    b: 2/3
    kwargs:
      tol: 1e-2
    error:
      type: ValueError
      message: no slope specified
